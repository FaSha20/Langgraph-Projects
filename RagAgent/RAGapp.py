
import streamlit as st
import os
from ragAgentt import *
import pandas as pd

st.set_page_config(page_title="FAQ RAG Assistant ğŸ¤–", layout="wide")

st.title("FAQ (RAG Assistant) ğŸ’¬")
st.markdown("Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø´Ù…Ø§ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ù…ØªØ¯Ø§ÙˆÙ„ (FAQ) Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ù….")

# --- File Paths and Initial Setup ---
# Define your actual JSON file path here for the RAG system
json_file_path = r".\dobare_faqs.json"

# --- RAG Initialization (cached) ---
@st.cache_resource # Caching the initialization of RAG components
def initialize_rag_components(file_path):
    st.write("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… RAG... (Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ù„Ø­Ø¸Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)")
    documents = load_and_chunk_json_faqs(file_path)
    st.success(f"âœ… {len(documents)} ØªÚ©Ù‡ (chunk) Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.")
    vectorstore, embeddings = setup_vector_store(documents, EMBEDDING_MODEL_NAME, VECTOR_DB_PATH)
    st.success("âœ… Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù…Ø¯Ù„ ØªØ¹Ø¨ÛŒÙ‡ (embedding) Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÙ†Ø¯.")
    persian_prompt = create_persian_prompt_template()
    st.success("âœ… Ø§Ù„Ú¯ÙˆÛŒ (Prompt) ÙØ§Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯.")
    return vectorstore, persian_prompt

# Call the initialization function
vectorstore, persian_prompt = initialize_rag_components(json_file_path)

st.divider()

# --- User Input ---
user_query = st.text_input("Ø³Ø¤Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:", placeholder="Ù…Ø«Ø§Ù„: Ø³Ø§Ø¹Ø§Øª Ú©Ø§Ø±ÛŒ Ø´Ù…Ø§ Ú†ÛŒØ³ØªØŸ")

if user_query:
    st.info(f"Ø´Ù…Ø§ Ù¾Ø±Ø³ÛŒØ¯ÛŒØ¯: **{user_query}**")
    
    try:
        # Call the RAG pipeline with user's query
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®..."):
            answer, retrieved_context = \
                rag_pipeline(user_query, vectorstore, persian_prompt)
        
        st.success("Ù¾Ø§Ø³Ø® ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯:")
        st.write(f"**Ù¾Ø§Ø³Ø®:** {answer}")

        st.divider()

        # --- Display Retrieved Chunks ---
        st.subheader("ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡ (Retrieved Chunks) ğŸ“š")
        if retrieved_context:
            for i, doc in enumerate(retrieved_context):
                with st.expander(f"ØªÚ©Ù‡ #{i+1} (ID: {doc.metadata.get('question_id', 'N/A')})"):
                    st.markdown(f"**Ù…Ø­ØªÙˆØ§:** {doc.page_content}")
                    st.markdown(f"**Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ±Ø§Ø¯Ø§Ø¯Ù‡:** {json.dumps(doc.metadata, ensure_ascii=False, indent=2)}")
        else:
            st.warning("Ù‡ÛŒÚ† ØªÚ©Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù¾Ø±Ø³ Ùˆ Ø¬Ùˆ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†Ø´Ø¯.")

    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}")
        st.warning("Ù„Ø·ÙØ§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ Ú©Ù„ÛŒØ¯ API OpenAI Ø´Ù…Ø§ ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ ÙØ§ÛŒÙ„ JSON Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯.")

st.sidebar.header("Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡")
st.sidebar.info(
    "Ø§ÛŒÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø³ÛŒØ³ØªÙ… RAG (Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ-ØªÙ‚ÙˆÛŒØª-ØªÙˆÙ„ÛŒØ¯) Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ø³Ø¤Ø§Ù„Ø§Øª Ù…ØªØ¯Ø§ÙˆÙ„ (FAQ) Ø´Ù…Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. "
    "Ø¨Ø§ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø³Ø¤Ø§Ù„ØŒ Ø³ÛŒØ³ØªÙ… Ø§Ø¨ØªØ¯Ø§ ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ø§Ø² Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø±Ø¯Ù‡ØŒ Ø³Ù¾Ø³ Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ø§ Ú©Ù…Ú© ÛŒÚ© Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¨Ø²Ø±Ú¯ (LLM) ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
)